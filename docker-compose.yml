version: '3.8'

services:
  drumscore-backend:
    build:
      context: .
      dockerfile: Dockerfile
    platform: linux/amd64  # Force Linux AMD64 platform (required for TensorFlow 2.5.0 on macOS)
    container_name: drumscore-backend
    ports:
      - "8000:8000"
    volumes:
      # Mount directories for persistent data
      - ./backend/uploads:/app/backend/uploads
      - ./backend/outputs:/app/backend/outputs
      - ./backend/logs:/app/backend/logs
      - ./backend/temp:/app/backend/temp
      # Mount model files (read-only) - only if models are in backend/models/
      - ./AnNOTEator/inference/pretrained_models:/app/AnNOTEator/inference/pretrained_models:ro
    environment:
      # Server settings
      - HOST=0.0.0.0
      - PORT=8000
      - DEBUG=false
      # Processing settings (change to 'cuda' if using GPU)
      - DEMUCS_DEVICE=cpu
      - OMNIZART_DEVICE=cpu
      # Concurrency
      - MAX_CONCURRENT_JOBS=2
      # Logging
      - LOG_LEVEL=INFO
      # Fix protobuf version mismatch (TensorFlow 2.5.0 compiled with 3.9.2, runtime has 3.20.3)
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/v1/health', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Resource limits (adjust based on your needs)
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    # Uncomment for GPU support (requires nvidia-docker)
    # runtime: nvidia
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  default:
    name: drumscore-network
